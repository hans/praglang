"""
Defines recurrent decoder Lasagne layers.
"""

import theano
from theano import tensor as T

from lasagne import layers as L
from lasagne import nonlinearities as NL
from lasagne.layers import helper, MergeLayer
from lasagne.layers.recurrent import Gate


class GRUDecoderLayer(MergeLayer):
    r"""
    lasagne.layers.recurrent.GRULayer(incoming, num_units,
    resetgate=lasagne.layers.Gate(W_cell=None),
    updategate=lasagne.layers.Gate(W_cell=None),
    hidden_update=lasagne.layers.Gate(
    W_cell=None, lasagne.nonlinearities.tanh),
    hid_init=lasagne.init.Constant(0.), backwards=False, learn_init=False,
    gradient_steps=-1, grad_clipping=0, unroll_scan=False,
    precompute_input=True, mask_input=None, only_return_final=False, **kwargs)
    Gated Recurrent Unit (GRU) Layer
    Implements the recurrent step proposed in [1]_, which computes the output
    by
    .. math ::
        r_t &= \sigma_r(x_t W_{xr} + h_{t - 1} W_{hr} + b_r)\\
        u_t &= \sigma_u(x_t W_{xu} + h_{t - 1} W_{hu} + b_u)\\
        c_t &= \sigma_c(x_t W_{xc} + r_t \odot (h_{t - 1} W_{hc}) + b_c)\\
        h_t &= (1 - u_t) \odot h_{t - 1} + u_t \odot c_t
    Parameters
    ----------
    incoming : a :class:`lasagne.layers.Layer` instance or a tuple
        The layer feeding into this layer, or the expected input shape.
    num_units : int
        Number of hidden units in the layer.
    resetgate : Gate
        Parameters for the reset gate (:math:`r_t`): :math:`W_{xr}`,
        :math:`W_{hr}`, :math:`b_r`, and :math:`\sigma_r`.
    updategate : Gate
        Parameters for the update gate (:math:`u_t`): :math:`W_{xu}`,
        :math:`W_{hu}`, :math:`b_u`, and :math:`\sigma_u`.
    hidden_update : Gate
        Parameters for the hidden update (:math:`c_t`): :math:`W_{xc}`,
        :math:`W_{hc}`, :math:`b_c`, and :math:`\sigma_c`.
    hid_init : callable, np.ndarray, theano.shared or :class:`Layer`
        Initializer for initial hidden state (:math:`h_0`).
    backwards : bool
        If True, process the sequence backwards and then reverse the
        output again such that the output from the layer is always
        from :math:`x_1` to :math:`x_n`.
    learn_init : bool
        If True, initial hidden values are learned.
    gradient_steps : int
        Number of timesteps to include in the backpropagated gradient.
        If -1, backpropagate through the entire sequence.
    grad_clipping : float
        If nonzero, the gradient messages are clipped to the given value during
        the backward pass.  See [1]_ (p. 6) for further explanation.
    unroll_scan : bool
        If True the recursion is unrolled instead of using scan. For some
        graphs this gives a significant speed up but it might also consume
        more memory. When `unroll_scan` is True, backpropagation always
        includes the full sequence, so `gradient_steps` must be set to -1 and
        the input sequence length must be known at compile time (i.e., cannot
        be given as None).
    mask_input : :class:`lasagne.layers.Layer`
        Layer which allows for a sequence mask to be input, for when sequences
        are of variable length.  Default `None`, which means no mask will be
        supplied (i.e. all sequences are of the same length).
    References
    ----------
    .. [1] Cho, Kyunghyun, et al: On the properties of neural
       machine translation: Encoder-decoder approaches.
       arXiv preprint arXiv:1409.1259 (2014).
    .. [2] Chung, Junyoung, et al.: Empirical Evaluation of Gated
       Recurrent Neural Networks on Sequence Modeling.
       arXiv preprint arXiv:1412.3555 (2014).
    .. [3] Graves, Alex: "Generating sequences with recurrent neural networks."
           arXiv preprint arXiv:1308.0850 (2013).
    Notes
    -----
    An alternate update for the candidate hidden state is proposed in [2]_:
    .. math::
        c_t &= \sigma_c(x_t W_{ic} + (r_t \odot h_{t - 1})W_{hc} + b_c)\\
    We use the formulation from [1]_ because it allows us to do all matrix
    operations in a single dot product.
    """
    def __init__(self, incoming_state, num_units,
                 num_timesteps, l_emb, l_out,
                 resetgate=Gate(W_cell=None),
                 updategate=Gate(W_cell=None),
                 hidden_update=Gate(W_cell=None,
                                    nonlinearity=NL.tanh),
                 backwards=False,
                 learn_init=False,
                 gradient_steps=-1,
                 grad_clipping=0,
                 unroll_scan=False,
                 mask_input=None,
                 **kwargs):

        # Always at least one input: incoming state from preceding layer
        incomings = [incoming_state]
        self.mask_incoming_index = -1
        if mask_input is not None:
            incomings.append(mask_input)
            self.mask_incoming_index = len(incomings)-1

        # Initialize parent layer
        super(GRUDecoderLayer, self).__init__(incomings, **kwargs)

        self.num_timesteps = num_timesteps
        self.l_emb = l_emb
        self.l_out = l_out

        self.learn_init = learn_init
        self.num_units = num_units
        self.grad_clipping = grad_clipping
        self.backwards = backwards
        self.gradient_steps = gradient_steps
        self.unroll_scan = unroll_scan

        if unroll_scan and gradient_steps != -1:
            raise ValueError(
                "Gradient steps must be -1 when unroll_scan is true.")

        # Shape prep
        self.input_dim = self.l_emb.output_shape[-1]
        self.vocab_size = self.l_out.output_shape[1]
        assert self.l_out.input_shape[1] == self.num_units

        def add_gate_params(gate, gate_name):
            """ Convenience function for adding layer parameters from a Gate
            instance. """
            return (self.add_param(gate.W_in, (self.input_dim, num_units),
                                   name="W_in_to_{}".format(gate_name)),
                    self.add_param(gate.W_hid, (num_units, num_units),
                                   name="W_hid_to_{}".format(gate_name)),
                    self.add_param(gate.b, (num_units,),
                                   name="b_{}".format(gate_name),
                                   regularizable=False),
                    gate.nonlinearity)

        # Add in all parameters from gates
        (self.W_in_to_updategate, self.W_hid_to_updategate, self.b_updategate,
         self.nonlinearity_updategate) = add_gate_params(updategate,
                                                         'updategate')
        (self.W_in_to_resetgate, self.W_hid_to_resetgate, self.b_resetgate,
         self.nonlinearity_resetgate) = add_gate_params(resetgate, 'resetgate')

        (self.W_in_to_hidden_update, self.W_hid_to_hidden_update,
         self.b_hidden_update, self.nonlinearity_hid) = add_gate_params(
             hidden_update, 'hidden_update')

        # Stack input weight matrices into a (num_inputs, 3*num_units)
        # matrix, which speeds up computation
        self.W_in_stacked = T.concatenate(
            [self.W_in_to_resetgate, self.W_in_to_updategate,
             self.W_in_to_hidden_update], axis=1)

        # Same for hidden weight matrices
        self.W_hid_stacked = T.concatenate(
            [self.W_hid_to_resetgate, self.W_hid_to_updategate,
             self.W_hid_to_hidden_update], axis=1)

        # Stack gate biases into a (3*num_units) vector
        self.b_stacked = T.concatenate(
            [self.b_resetgate, self.b_updategate,
             self.b_hidden_update], axis=0)

    def get_params(self, **tags):
        params = super(GRUDecoderLayer, self).get_params(**tags)

        # Also include params of subordinate layers
        params += self.l_emb.get_params(**tags) # TODO will be duplicate?
        params += self.l_out.get_params(**tags)
        return params

    def get_output_shape_for(self, input_shapes):
        incoming_state_shape = input_shapes[0]
        batch_size = incoming_state_shape[0]

        return batch_size, self.num_timesteps, self.num_units

    def _gru_step(self, x, h):
        # When theano.scan calls step, input_n will be (n_batch, 3*num_units).
        # We define a slicing function that extract the input to each GRU gate
        def slice_w(x, n):
            return x[:, n*self.num_units:(n+1)*self.num_units]

        x = helper.get_output(self.l_emb, x)
        x_gates = T.dot(x, self.W_in_stacked) + self.b_stacked

        hid_gates = T.dot(h, self.W_hid_stacked)

        if self.grad_clipping:
            x_gates = theano.gradient.grad_clip(
                x_gates, -self.grad_clipping, self.grad_clipping)
            hid_gates = theano.gradient.grad_clip(
                hid_gates, -self.grad_clipping, self.grad_clipping)

        # Reset and update gates
        r = slice_w(hid_gates, 0) + slice_w(x_gates, 0)
        u = slice_w(hid_gates, 1) + slice_w(x_gates, 1)
        r = self.nonlinearity_resetgate(r)
        u = self.nonlinearity_updategate(u)

        hidden_update = slice_w(x_gates, 2) + r * slice_w(hid_gates, 2)
        if self.grad_clipping:
            hidden_update = theano.gradient.grad_clip(
                hidden_update, -self.grad_clipping, self.grad_clipping)
        hidden_update = self.nonlinearity_hid(hidden_update)

        # Compute (1 - u_t)h_{t - 1} + u_t c_t
        hid = (1 - u) * h + u * hidden_update

        # Compute output distribution
        out = helper.get_output(self.l_out, hid)

        return out, hid


    class GRUDecoderStepLayer(MergeLayer):
        def __init__(self, decoder, **kwargs):
            incomings = [L.InputLayer(shape=(None,),
                                      input_var=T.ivector(), name="gru_step_x"),
                         L.InputLayer(shape=(None, decoder.num_units),
                                      input_var=T.matrix(), name="gru_step_h")]
            super(GRUDecoderLayer.GRUDecoderStepLayer, self).__init__(incomings, **kwargs)

            self.x_in, self.h_prev_in = incomings
            self.decoder = decoder

        def get_output_shape_for(self, input_shapes):
            x_shape, h_shape = input_shapes
            assert x_shape[0] == h_shape[0]
            return h_shape, x_shape

        def get_output_for(self, inputs, **kwargs):
            x, h = inputs
            return self.decoder._gru_step(x, h)

    def get_step_layer(self, name=None):
        return self.GRUDecoderStepLayer(self, name=name or "%s/step" % self.name)


    def get_output_for(self, inputs, **kwargs):
        """
        Compute this layer's output function given a symbolic input variable
        Parameters
        ----------
        inputs : list of theano.TensorType
            `inputs[0]` should always be the symbolic input variable.  When
            this layer has a mask input (i.e. was instantiated with
            `mask_input != None`, indicating that the lengths of sequences in
            each batch vary), `inputs` should have length 2, where `inputs[1]`
            is the `mask`.  The `mask` should be supplied as a Theano variable
            denoting whether each time step in each sequence in the batch is
            part of the sequence or not.  `mask` should be a matrix of shape
            ``(n_batch, n_time_steps)`` where ``mask[i, j] = 1`` when ``j <=
            (length of sequence i)`` and ``mask[i, j] = 0`` when ``j > (length
            of sequence i)``. When the hidden state of this layer is to be
            pre-filled (i.e. was set to a :class:`Layer` instance) `inputs`
            should have length at least 2, and `inputs[-1]` is the hidden state
            to prefill with.
        Returns
        -------
        layer_output : theano.TensorType
            Symbolic output variable.
        """
        # Retrieve input encoding
        hid_init = inputs[0]
        # Retrieve the mask when it is supplied
        mask = None
        if self.mask_incoming_index > 0:
            mask = inputs[self.mask_incoming_index]

        batch_size = hid_init.shape[0]

        # Create single recurrent computation step function
        # input__n is the n'th vector of the input
        def step(out_previous, hid_previous, *args):
            # Look up embedding vectors from previous output.
            emb_idxs = T.argmax(out_previous, axis=1)

            out, hid = self._gru_step(emb_idxs, hid_previous)
            return out, hid

        def step_masked(mask_n, out_previous, hid_previous, *args):
            out, hid = step(out_previous, hid_previous, *args)

            # Skip over any input with mask 0 by copying the previous
            # hidden state; proceed normally for any input with mask 1.
            hid = T.switch(mask_n, hid, hid_previous)
            out = T.switch(mask_n, out, out_previous)

            return out, hid

        if mask is not None:
            # mask is given as (batch_size, seq_len). Because scan iterates
            # over first dimension, we dimshuffle to (seq_len, batch_size) and
            # add a broadcastable dimension
            mask = mask.dimshuffle(1, 0, 'x')
            sequences = [mask]
            step_fun = step_masked
        else:
            sequences = []
            step_fun = step

        non_seqs = self.get_params()

        # TODO customize -- assumes that idx 0 == start token
        out_init = T.zeros((batch_size, self.vocab_size))

        if self.unroll_scan:
            # Explicitly unroll the recurrence instead of using scan
            out, _ = unroll_scan(
                fn=step_fun,
                sequences=sequences,
                outputs_info=[out_init, hid_init],
                go_backwards=self.backwards,
                non_sequences=non_seqs,
                n_steps=self.num_timesteps)[0]
        else:
            # Scan op iterates over first dimension of input and repeatedly
            # applies the step function
            out, _ = theano.scan(
                fn=step_fun,
                sequences=sequences,
                go_backwards=self.backwards,
                outputs_info=[out_init, hid_init],
                non_sequences=non_seqs,
                truncate_gradient=self.gradient_steps,
                n_steps=self.num_timesteps,
                strict=True)[0]

        # dimshuffle back to (n_batch, n_timesteps, vocab_size)
        out = out.dimshuffle(1, 0, 2)

        # if scan is backward reverse the output
        if self.backwards:
            out = out[:, ::-1]

        return out
